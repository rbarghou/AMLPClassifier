{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import amlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = amlp.AMLPClassifier(\n",
    "    hidden_layer_size_ranges=((10, 50), (5, 30), (10, 20), (10, 20), (10, 20), (10, 20)),\n",
    "    verbose=True,\n",
    ")\n",
    "k = 5\n",
    "X = np.random.randn(30000, k)\n",
    "y = np.ravel(\n",
    "    np.sin(100. / np.dot(\n",
    "        X * X,\n",
    "        np.ones((k, 1))\n",
    "    ) ** 2) > .25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69364843\n",
      "Iteration 2, loss = 0.64005075\n",
      "Iteration 3, loss = 0.58186634\n",
      "Iteration 4, loss = 0.54728565\n",
      "Iteration 5, loss = 0.53016342\n",
      "Iteration 6, loss = 0.51653247\n",
      "Iteration 7, loss = 0.50714826\n",
      "Iteration 8, loss = 0.49803222\n",
      "Iteration 9, loss = 0.48890775\n",
      "Iteration 10, loss = 0.48081766\n",
      "Iteration 11, loss = 0.47531609\n",
      "Iteration 12, loss = 0.46896800\n",
      "Iteration 13, loss = 0.46520901\n",
      "Iteration 14, loss = 0.45989298\n",
      "Iteration 15, loss = 0.45516191\n",
      "Iteration 16, loss = 0.45169285\n",
      "Iteration 17, loss = 0.44866311\n",
      "Iteration 18, loss = 0.44709194\n",
      "Iteration 19, loss = 0.44413839\n",
      "Iteration 20, loss = 0.44167594\n",
      "Iteration 21, loss = 0.44003811\n",
      "Iteration 22, loss = 0.43798319\n",
      "Iteration 23, loss = 0.43672006\n",
      "Iteration 24, loss = 0.43466276\n",
      "Iteration 25, loss = 0.43280848\n",
      "Iteration 26, loss = 0.43226686\n",
      "Iteration 27, loss = 0.42864156\n",
      "Iteration 28, loss = 0.42588093\n",
      "Iteration 29, loss = 0.42422495\n",
      "Iteration 30, loss = 0.42079526\n",
      "Iteration 31, loss = 0.41919927\n",
      "Iteration 32, loss = 0.41629794\n",
      "Iteration 33, loss = 0.41335195\n",
      "Iteration 34, loss = 0.41168216\n",
      "Iteration 35, loss = 0.40865945\n",
      "Iteration 36, loss = 0.40428682\n",
      "Iteration 37, loss = 0.40223963\n",
      "Iteration 38, loss = 0.39851565\n",
      "Iteration 39, loss = 0.39551276\n",
      "Iteration 40, loss = 0.39204308\n",
      "Iteration 41, loss = 0.39179579\n",
      "Iteration 42, loss = 0.38862090\n",
      "Iteration 43, loss = 0.38691955\n",
      "Iteration 44, loss = 0.38509758\n",
      "Iteration 45, loss = 0.38416773\n",
      "Iteration 46, loss = 0.38281774\n",
      "Iteration 47, loss = 0.38216843\n",
      "Iteration 48, loss = 0.37997906\n",
      "Iteration 49, loss = 0.37916975\n",
      "Iteration 50, loss = 0.37843143\n",
      "Iteration 51, loss = 0.37703159\n",
      "Iteration 52, loss = 0.37556515\n",
      "Iteration 53, loss = 0.37489378\n",
      "Iteration 54, loss = 0.37315405\n",
      "Iteration 55, loss = 0.37177954\n",
      "Iteration 56, loss = 0.37094275\n",
      "Iteration 57, loss = 0.37002083\n",
      "Iteration 58, loss = 0.36918840\n",
      "Iteration 59, loss = 0.36952890\n",
      "Iteration 60, loss = 0.36811543\n",
      "Iteration 61, loss = 0.36909115\n",
      "Iteration 62, loss = 0.36728194\n",
      "Iteration 63, loss = 0.36541958\n",
      "Iteration 64, loss = 0.36504135\n",
      "Iteration 65, loss = 0.36302620\n",
      "Iteration 66, loss = 0.36294339\n",
      "Iteration 67, loss = 0.36116875\n",
      "Iteration 68, loss = 0.35965659\n",
      "Iteration 69, loss = 0.36115332\n",
      "Iteration 70, loss = 0.35753647\n",
      "Iteration 71, loss = 0.35671506\n",
      "Iteration 72, loss = 0.35611293\n",
      "Iteration 73, loss = 0.35442699\n",
      "Iteration 74, loss = 0.35312650\n",
      "Iteration 75, loss = 0.35208113\n",
      "Iteration 76, loss = 0.35417663\n",
      "Iteration 77, loss = 0.35074999\n",
      "Iteration 78, loss = 0.34863955\n",
      "Iteration 79, loss = 0.34743437\n",
      "Iteration 80, loss = 0.34680416\n",
      "Iteration 81, loss = 0.34534360\n",
      "Iteration 82, loss = 0.34609918\n",
      "Iteration 83, loss = 0.34400953\n",
      "Iteration 84, loss = 0.34411151\n",
      "Iteration 85, loss = 0.34347405\n",
      "Iteration 86, loss = 0.34452147\n",
      "Iteration 87, loss = 0.34211624\n",
      "Iteration 88, loss = 0.33948810\n",
      "Iteration 89, loss = 0.34149045\n",
      "Iteration 90, loss = 0.33952040\n",
      "Iteration 91, loss = 0.34073281\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(11, 6, 11, 11, 11, 11)\n",
      "Iteration 92, loss = 0.33802529\n",
      "Iteration 93, loss = 0.33582890\n",
      "Iteration 94, loss = 0.33632350\n",
      "Iteration 95, loss = 0.33643672\n",
      "Iteration 96, loss = 0.33476034\n",
      "Iteration 97, loss = 0.33401379\n",
      "Iteration 98, loss = 0.33501527\n",
      "Iteration 99, loss = 0.33306824\n",
      "Iteration 100, loss = 0.33052499\n",
      "Iteration 101, loss = 0.33299539\n",
      "Iteration 102, loss = 0.33075016\n",
      "Iteration 103, loss = 0.32857033\n",
      "Iteration 104, loss = 0.33203513\n",
      "Iteration 105, loss = 0.33066285\n",
      "Iteration 106, loss = 0.32841653\n",
      "Iteration 107, loss = 0.32701820\n",
      "Iteration 108, loss = 0.32817446\n",
      "Iteration 109, loss = 0.32752736\n",
      "Iteration 110, loss = 0.32511502\n",
      "Iteration 111, loss = 0.32419464\n",
      "Iteration 112, loss = 0.32567097\n",
      "Iteration 113, loss = 0.32390361\n",
      "Iteration 114, loss = 0.32327449\n",
      "Iteration 115, loss = 0.32466866\n",
      "Iteration 116, loss = 0.32400418\n",
      "Iteration 117, loss = 0.32386956\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(12, 7, 12, 12, 12, 12)\n",
      "Iteration 118, loss = 0.32349886\n",
      "Iteration 119, loss = 0.32133866\n",
      "Iteration 120, loss = 0.32181959\n",
      "Iteration 121, loss = 0.32091850\n",
      "Iteration 122, loss = 0.31988217\n",
      "Iteration 123, loss = 0.32039345\n",
      "Iteration 124, loss = 0.31987651\n",
      "Iteration 125, loss = 0.31928530\n",
      "Iteration 126, loss = 0.31883031\n",
      "Iteration 127, loss = 0.31750116\n",
      "Iteration 128, loss = 0.31813018\n",
      "Iteration 129, loss = 0.31708435\n",
      "Iteration 130, loss = 0.31566547\n",
      "Iteration 131, loss = 0.31508612\n",
      "Iteration 132, loss = 0.31283318\n",
      "Iteration 133, loss = 0.31569906\n",
      "Iteration 134, loss = 0.31122863\n",
      "Iteration 135, loss = 0.31007970\n",
      "Iteration 136, loss = 0.30973918\n",
      "Iteration 137, loss = 0.31061218\n",
      "Iteration 138, loss = 0.30791835\n",
      "Iteration 139, loss = 0.30806603\n",
      "Iteration 140, loss = 0.30814603\n",
      "Iteration 141, loss = 0.30955340\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(13, 8, 13, 13, 13, 13)\n",
      "Iteration 142, loss = 0.30747290\n",
      "Iteration 143, loss = 0.30762477\n",
      "Iteration 144, loss = 0.30755573\n",
      "Iteration 145, loss = 0.30563848\n",
      "Iteration 146, loss = 0.30399259\n",
      "Iteration 147, loss = 0.30651193\n",
      "Iteration 148, loss = 0.30535405\n",
      "Iteration 149, loss = 0.30324004\n",
      "Iteration 150, loss = 0.30419241\n",
      "Iteration 151, loss = 0.30229176\n",
      "Iteration 152, loss = 0.30240421\n",
      "Iteration 153, loss = 0.30281494\n",
      "Iteration 154, loss = 0.30183136\n",
      "Iteration 155, loss = 0.30273791\n",
      "Iteration 156, loss = 0.30278090\n",
      "Iteration 157, loss = 0.30121039\n",
      "Iteration 158, loss = 0.29987664\n",
      "Iteration 159, loss = 0.30179963\n",
      "Iteration 160, loss = 0.30093810\n",
      "Iteration 161, loss = 0.30232482\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(14, 9, 14, 14, 14, 14)\n",
      "Iteration 162, loss = 0.30000336\n",
      "Iteration 163, loss = 0.29985442\n",
      "Iteration 164, loss = 0.30008262\n",
      "Iteration 165, loss = 0.29969707\n",
      "Iteration 166, loss = 0.29984713\n",
      "Iteration 167, loss = 0.30009646\n",
      "Iteration 168, loss = 0.29903709\n",
      "Iteration 169, loss = 0.30027829\n",
      "Iteration 170, loss = 0.29996475\n",
      "Iteration 171, loss = 0.29986328\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(15, 10, 15, 15, 15, 15)\n",
      "Iteration 172, loss = 0.29984350\n",
      "Iteration 173, loss = 0.29994301\n",
      "Iteration 174, loss = 0.29903387\n",
      "Iteration 175, loss = 0.29872105\n",
      "Iteration 176, loss = 0.29859953\n",
      "Iteration 177, loss = 0.29728865\n",
      "Iteration 178, loss = 0.29854661\n",
      "Iteration 179, loss = 0.29995692\n",
      "Iteration 180, loss = 0.29778403\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(16, 11, 16, 16, 16, 16)\n",
      "Iteration 181, loss = 0.29853955\n",
      "Iteration 182, loss = 0.29983867\n",
      "Iteration 183, loss = 0.29735502\n",
      "Iteration 184, loss = 0.29711981\n",
      "Iteration 185, loss = 0.29845750\n",
      "Iteration 186, loss = 0.29853371\n",
      "Iteration 187, loss = 0.29723097\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(17, 12, 17, 17, 17, 17)\n",
      "Iteration 188, loss = 0.29818611\n",
      "Iteration 189, loss = 0.29683155\n",
      "Iteration 190, loss = 0.29864253\n",
      "Iteration 191, loss = 0.29690661\n",
      "Iteration 192, loss = 0.29401617\n",
      "Iteration 193, loss = 0.29557458\n",
      "Iteration 194, loss = 0.29551101\n",
      "Iteration 195, loss = 0.29595972\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(18, 13, 18, 18, 18, 18)\n",
      "Iteration 196, loss = 0.29636580\n",
      "Iteration 197, loss = 0.29406129\n",
      "Iteration 198, loss = 0.29520458\n",
      "Iteration 199, loss = 0.29545557\n",
      "Iteration 200, loss = 0.29461093\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(19, 14, 19, 19, 19, 19)\n",
      "Iteration 201, loss = 0.29690563\n",
      "Iteration 202, loss = 0.29429706\n",
      "Iteration 203, loss = 0.29684160\n",
      "Iteration 204, loss = 0.29613065\n",
      "Iteration 205, loss = 0.29276553\n",
      "Iteration 206, loss = 0.29551983\n",
      "Iteration 207, loss = 0.29571532\n",
      "Iteration 208, loss = 0.29203876\n",
      "Iteration 209, loss = 0.29367055\n",
      "Iteration 210, loss = 0.29252581\n",
      "Iteration 211, loss = 0.29391593\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(20, 15, 20, 20, 20, 20)\n",
      "Iteration 212, loss = 0.29496557\n",
      "Iteration 213, loss = 0.29419818\n",
      "Iteration 214, loss = 0.29434653\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(21, 16, 21, 21, 21, 21)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 215, loss = 0.29398073\n",
      "Iteration 216, loss = 0.29494778\n",
      "Iteration 217, loss = 0.29264464\n",
      "Iteration 218, loss = 0.29238329\n",
      "Iteration 219, loss = 0.29373285\n",
      "Iteration 220, loss = 0.29346473\n",
      "Iteration 221, loss = 0.29397046\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(22, 17, 22, 22, 22, 22)\n",
      "Iteration 222, loss = 0.29275008\n",
      "Iteration 223, loss = 0.29545312\n",
      "Iteration 224, loss = 0.29232465\n",
      "Iteration 225, loss = 0.29269907\n",
      "Iteration 226, loss = 0.29064333\n",
      "Iteration 227, loss = 0.29390663\n",
      "Iteration 228, loss = 0.29175284\n",
      "Iteration 229, loss = 0.29252599\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(23, 18, 23, 23, 23, 23)\n",
      "Iteration 230, loss = 0.29271386\n",
      "Iteration 231, loss = 0.29281092\n",
      "Iteration 232, loss = 0.29167648\n",
      "Iteration 233, loss = 0.29294877\n",
      "Iteration 234, loss = 0.29260890\n",
      "Iteration 235, loss = 0.29383256\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(24, 19, 24, 24, 24, 24)\n",
      "Iteration 236, loss = 0.29368988\n",
      "Iteration 237, loss = 0.29231896\n",
      "Iteration 238, loss = 0.29215018\n",
      "Iteration 239, loss = 0.28969998\n",
      "Iteration 240, loss = 0.28902056\n",
      "Iteration 241, loss = 0.28591016\n",
      "Iteration 242, loss = 0.28670215\n",
      "Iteration 243, loss = 0.28539954\n",
      "Iteration 244, loss = 0.28589563\n",
      "Iteration 245, loss = 0.28429780\n",
      "Iteration 246, loss = 0.28178497\n",
      "Iteration 247, loss = 0.28268902\n",
      "Iteration 248, loss = 0.28297564\n",
      "Iteration 249, loss = 0.28092234\n",
      "Iteration 250, loss = 0.28262601\n",
      "Iteration 251, loss = 0.28063197\n",
      "Iteration 252, loss = 0.27968787\n",
      "Iteration 253, loss = 0.27920166\n",
      "Iteration 254, loss = 0.27984578\n",
      "Iteration 255, loss = 0.27780984\n",
      "Iteration 256, loss = 0.28126270\n",
      "Iteration 257, loss = 0.27695229\n",
      "Iteration 258, loss = 0.27633007\n",
      "Iteration 259, loss = 0.27850210\n",
      "Iteration 260, loss = 0.27905470\n",
      "Iteration 261, loss = 0.27680662\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(25, 20, 25, 25, 25, 25)\n",
      "Iteration 262, loss = 0.27598244\n",
      "Iteration 263, loss = 0.27496266\n",
      "Iteration 264, loss = 0.27601503\n",
      "Iteration 265, loss = 0.27563564\n",
      "Iteration 266, loss = 0.27618671\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(26, 21, 26, 26, 26, 26)\n",
      "Iteration 267, loss = 0.27490844\n",
      "Iteration 268, loss = 0.27288027\n",
      "Iteration 269, loss = 0.27498133\n",
      "Iteration 270, loss = 0.27357276\n",
      "Iteration 271, loss = 0.27364017\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(27, 22, 27, 27, 27, 27)\n",
      "Iteration 272, loss = 0.27448108\n",
      "Iteration 273, loss = 0.27152600\n",
      "Iteration 274, loss = 0.27273888\n",
      "Iteration 275, loss = 0.27439204\n",
      "Iteration 276, loss = 0.27407156\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(28, 23, 28, 28, 28, 28)\n",
      "Iteration 277, loss = 0.27403100\n",
      "Iteration 278, loss = 0.27229514\n",
      "Iteration 279, loss = 0.27386907\n",
      "Iteration 280, loss = 0.27299173\n",
      "Iteration 281, loss = 0.27287263\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(29, 24, 29, 29, 29, 29)\n",
      "Iteration 282, loss = 0.27328005\n",
      "Iteration 283, loss = 0.27326347\n",
      "Iteration 284, loss = 0.27190281\n",
      "Iteration 285, loss = 0.27116580\n",
      "Iteration 286, loss = 0.27220152\n",
      "Iteration 287, loss = 0.27286910\n",
      "Iteration 288, loss = 0.27040474\n",
      "Iteration 289, loss = 0.27096201\n",
      "Iteration 290, loss = 0.27285588\n",
      "Iteration 291, loss = 0.26978259\n",
      "Iteration 292, loss = 0.27157942\n",
      "Iteration 293, loss = 0.27074100\n",
      "Iteration 294, loss = 0.27133039\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(30, 25, 30, 30, 30, 30)\n",
      "Iteration 295, loss = 0.27088417\n",
      "Iteration 296, loss = 0.27007379\n",
      "Iteration 297, loss = 0.27078282\n",
      "Iteration 298, loss = 0.26959776\n",
      "Iteration 299, loss = 0.27047807\n",
      "Iteration 300, loss = 0.27078767\n",
      "Iteration 301, loss = 0.26841887\n",
      "Iteration 302, loss = 0.27056233\n",
      "Iteration 303, loss = 0.27036996\n",
      "Iteration 304, loss = 0.27030364\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(31, 26, 31, 31, 31, 31)\n",
      "Iteration 305, loss = 0.27047719\n",
      "Iteration 306, loss = 0.27002395\n",
      "Iteration 307, loss = 0.26866183\n",
      "Iteration 308, loss = 0.26827721\n",
      "Iteration 309, loss = 0.27000428\n",
      "Iteration 310, loss = 0.26695008\n",
      "Iteration 311, loss = 0.26713314\n",
      "Iteration 312, loss = 0.26780139\n",
      "Iteration 313, loss = 0.26730517\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(32, 27, 32, 32, 32, 32)\n",
      "Iteration 314, loss = 0.26816283\n",
      "Iteration 315, loss = 0.26631624\n",
      "Iteration 316, loss = 0.26626540\n",
      "Iteration 317, loss = 0.26608499\n",
      "Iteration 318, loss = 0.26489861\n",
      "Iteration 319, loss = 0.26569182\n",
      "Iteration 320, loss = 0.26909683\n",
      "Iteration 321, loss = 0.26623027\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(33, 28, 33, 33, 33, 33)\n",
      "Iteration 322, loss = 0.26787278\n",
      "Iteration 323, loss = 0.26496502\n",
      "Iteration 324, loss = 0.26431476\n",
      "Iteration 325, loss = 0.26509506\n",
      "Iteration 326, loss = 0.26609560\n",
      "Iteration 327, loss = 0.26534810\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(34, 29, 34, 34, 34, 34)\n",
      "Iteration 328, loss = 0.26620143\n",
      "Iteration 329, loss = 0.26660639\n",
      "Iteration 330, loss = 0.26264399\n",
      "Iteration 331, loss = 0.26400775\n",
      "Iteration 332, loss = 0.26653353\n",
      "Iteration 333, loss = 0.26468615\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(35, 30, 35, 35, 35, 35)\n",
      "Iteration 334, loss = 0.26530921\n",
      "Iteration 335, loss = 0.26264296\n",
      "Iteration 336, loss = 0.26265768\n",
      "Iteration 337, loss = 0.26298179\n",
      "Iteration 338, loss = 0.26499778\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(36, 31, 36, 36, 36, 36)\n",
      "Iteration 339, loss = 0.26385203\n",
      "Iteration 340, loss = 0.26303772\n",
      "Iteration 341, loss = 0.26402690\n",
      "Iteration 342, loss = 0.26267268\n",
      "Iteration 343, loss = 0.26326543\n",
      "Iteration 344, loss = 0.26168619\n",
      "Iteration 345, loss = 0.26125949\n",
      "Iteration 346, loss = 0.25985970\n",
      "Iteration 347, loss = 0.26270060\n",
      "Iteration 348, loss = 0.26126713\n",
      "Iteration 349, loss = 0.26107335\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(37, 32, 37, 37, 37, 37)\n",
      "Iteration 350, loss = 0.26196594\n",
      "Iteration 351, loss = 0.25924423\n",
      "Iteration 352, loss = 0.25919257\n",
      "Iteration 353, loss = 0.25924822\n",
      "Iteration 354, loss = 0.25914993\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(38, 33, 38, 38, 38, 38)\n",
      "Iteration 355, loss = 0.26018698\n",
      "Iteration 356, loss = 0.26033488\n",
      "Iteration 357, loss = 0.25801586\n",
      "Iteration 358, loss = 0.25890337\n",
      "Iteration 359, loss = 0.25844369\n",
      "Iteration 360, loss = 0.26015735\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(39, 34, 39, 39, 39, 39)\n",
      "Iteration 361, loss = 0.25873584\n",
      "Iteration 362, loss = 0.25610819\n",
      "Iteration 363, loss = 0.25593595\n",
      "Iteration 364, loss = 0.25676991\n",
      "Iteration 365, loss = 0.25691066\n",
      "Iteration 366, loss = 0.25917233\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(40, 35, 40, 40, 40, 40)\n",
      "Iteration 367, loss = 0.25738241\n",
      "Iteration 368, loss = 0.25775249\n",
      "Iteration 369, loss = 0.25555375\n",
      "Iteration 370, loss = 0.25761518\n",
      "Iteration 371, loss = 0.25513678\n",
      "Iteration 372, loss = 0.25498107\n",
      "Iteration 373, loss = 0.25595877\n",
      "Iteration 374, loss = 0.25733094\n",
      "Iteration 375, loss = 0.25332732\n",
      "Iteration 376, loss = 0.25481599\n",
      "Iteration 377, loss = 0.25543288\n",
      "Iteration 378, loss = 0.25418931\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(41, 36, 41, 41, 41, 41)\n",
      "Iteration 379, loss = 0.25713459\n",
      "Iteration 380, loss = 0.25470501\n",
      "Iteration 381, loss = 0.25392166\n",
      "Iteration 382, loss = 0.25263697\n",
      "Iteration 383, loss = 0.25343118\n",
      "Iteration 384, loss = 0.25576407\n",
      "Iteration 385, loss = 0.25147246\n",
      "Iteration 386, loss = 0.25381838\n",
      "Iteration 387, loss = 0.25441363\n",
      "Iteration 388, loss = 0.25322046\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(42, 37, 42, 42, 42, 42)\n",
      "Iteration 389, loss = 0.25434806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 390, loss = 0.25385683\n",
      "Iteration 391, loss = 0.25320394\n",
      "Iteration 392, loss = 0.25295089\n",
      "Iteration 393, loss = 0.25202331\n",
      "Iteration 394, loss = 0.25127360\n",
      "Iteration 395, loss = 0.25119102\n",
      "Iteration 396, loss = 0.25074552\n",
      "Iteration 397, loss = 0.25186496\n",
      "Iteration 398, loss = 0.25038585\n",
      "Iteration 399, loss = 0.24895984\n",
      "Iteration 400, loss = 0.25054650\n",
      "Iteration 401, loss = 0.25020583\n",
      "Iteration 402, loss = 0.25035913\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(43, 38, 43, 43, 43, 43)\n",
      "Iteration 403, loss = 0.25110313\n",
      "Iteration 404, loss = 0.24676860\n",
      "Iteration 405, loss = 0.25053865\n",
      "Iteration 406, loss = 0.24911383\n",
      "Iteration 407, loss = 0.25040570\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(44, 39, 44, 44, 44, 44)\n",
      "Iteration 408, loss = 0.24877144\n",
      "Iteration 409, loss = 0.24953872\n",
      "Iteration 410, loss = 0.24722717\n",
      "Iteration 411, loss = 0.24852319\n",
      "Iteration 412, loss = 0.24582590\n",
      "Iteration 413, loss = 0.24874481\n",
      "Iteration 414, loss = 0.24856605\n",
      "Iteration 415, loss = 0.24773597\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(45, 40, 45, 45, 45, 45)\n",
      "Iteration 416, loss = 0.24825124\n",
      "Iteration 417, loss = 0.24643059\n",
      "Iteration 418, loss = 0.24975448\n",
      "Iteration 419, loss = 0.24814601\n",
      "Iteration 420, loss = 0.24597952\n",
      "Iteration 421, loss = 0.24705870\n",
      "Iteration 422, loss = 0.24672717\n",
      "Iteration 423, loss = 0.24694476\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(46, 41, 46, 46, 46, 46)\n",
      "Iteration 424, loss = 0.24818947\n",
      "Iteration 425, loss = 0.24788648\n",
      "Iteration 426, loss = 0.24696757\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(47, 42, 47, 47, 47, 47)\n",
      "Iteration 427, loss = 0.24812138\n",
      "Iteration 428, loss = 0.24640463\n",
      "Iteration 429, loss = 0.24913874\n",
      "Iteration 430, loss = 0.24521967\n",
      "Iteration 431, loss = 0.24592091\n",
      "Iteration 432, loss = 0.24817036\n",
      "Iteration 433, loss = 0.24644442\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(48, 43, 48, 48, 48, 48)\n",
      "Iteration 434, loss = 0.24929196\n",
      "Iteration 435, loss = 0.24555783\n",
      "Iteration 436, loss = 0.24638974\n",
      "Iteration 437, loss = 0.24520625\n",
      "Iteration 438, loss = 0.24580534\n",
      "Iteration 439, loss = 0.24767401\n",
      "Iteration 440, loss = 0.24707265\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(49, 44, 49, 49, 49, 49)\n",
      "Iteration 441, loss = 0.24692979\n",
      "Iteration 442, loss = 0.24486337\n",
      "Iteration 443, loss = 0.24749792\n",
      "Iteration 444, loss = 0.24776434\n",
      "Iteration 445, loss = 0.24591358\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "(50, 45, 50, 50, 50, 50)\n",
      "Iteration 446, loss = 0.24559768\n",
      "Iteration 447, loss = 0.24354786\n",
      "Iteration 448, loss = 0.24538470\n",
      "Iteration 449, loss = 0.24724354\n",
      "Iteration 450, loss = 0.24362751\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AMLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "        beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "        hidden_layer_size_ranges=((10, 50), (5, 30), (10, 20), (10, 20), (10, 20), (10, 20)),\n",
       "        learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
       "        momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "        random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "        validation_fraction=0.1, verbose=True, warm_start=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x110e49850>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCSg/w9YhJmKyWXyVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110d2a910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(model.loss_curve_).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87403333333333333"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.predict(X) == y) * 1. / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "_X = np.random.randn(30000, k)\n",
    "_y = np.ravel(\n",
    "    np.sin(100. / np.dot(\n",
    "        _X * _X,\n",
    "        np.ones((k, 1))\n",
    "    ) ** 2) > .25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86246666666666671"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.predict(_X) == _y) * 1. / len(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(50, 45, 50, 50, 50, 50),\n",
    "    verbose=True,\n",
    "    warm_start=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56725863\n",
      "Iteration 2, loss = 0.45749923\n",
      "Iteration 3, loss = 0.40217907\n",
      "Iteration 4, loss = 0.38194193\n",
      "Iteration 5, loss = 0.36565368\n",
      "Iteration 6, loss = 0.35442971\n",
      "Iteration 7, loss = 0.34012123\n",
      "Iteration 8, loss = 0.32858799\n",
      "Iteration 9, loss = 0.32246258\n",
      "Iteration 10, loss = 0.31984831\n",
      "Iteration 11, loss = 0.31468432\n",
      "Iteration 12, loss = 0.30529168\n",
      "Iteration 13, loss = 0.30702105\n",
      "Iteration 14, loss = 0.30368701\n",
      "Iteration 15, loss = 0.29220277\n",
      "Iteration 16, loss = 0.30642248\n",
      "Iteration 17, loss = 0.28533013\n",
      "Iteration 18, loss = 0.28337497\n",
      "Iteration 19, loss = 0.28055862\n",
      "Iteration 20, loss = 0.27932632\n",
      "Iteration 21, loss = 0.28232927\n",
      "Iteration 22, loss = 0.27522096\n",
      "Iteration 23, loss = 0.27019936\n",
      "Iteration 24, loss = 0.27066212\n",
      "Iteration 25, loss = 0.26670623\n",
      "Iteration 26, loss = 0.26615771\n",
      "Iteration 27, loss = 0.26106633\n",
      "Iteration 28, loss = 0.26800502\n",
      "Iteration 29, loss = 0.26542696\n",
      "Iteration 30, loss = 0.26018873\n",
      "Iteration 31, loss = 0.25794930\n",
      "Iteration 32, loss = 0.25715610\n",
      "Iteration 33, loss = 0.25685858\n",
      "Iteration 34, loss = 0.25172800\n",
      "Iteration 35, loss = 0.26057986\n",
      "Iteration 36, loss = 0.25621117\n",
      "Iteration 37, loss = 0.25112703\n",
      "Iteration 38, loss = 0.24987443\n",
      "Iteration 39, loss = 0.24805066\n",
      "Iteration 40, loss = 0.24712511\n",
      "Iteration 41, loss = 0.24685195\n",
      "Iteration 42, loss = 0.24558050\n",
      "Iteration 43, loss = 0.24873565\n",
      "Iteration 44, loss = 0.24212816\n",
      "Iteration 45, loss = 0.24353331\n",
      "Iteration 46, loss = 0.24078223\n",
      "Iteration 47, loss = 0.24368824\n",
      "Iteration 48, loss = 0.23526058\n",
      "Iteration 49, loss = 0.24249449\n",
      "Iteration 50, loss = 0.23693103\n",
      "Iteration 51, loss = 0.23778479\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50, 45, 50, 50, 50, 50),\n",
       "       learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.23312845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50, 45, 50, 50, 50, 50),\n",
       "       learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_model.pre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
